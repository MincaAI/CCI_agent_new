import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.vectorstores import Pinecone
from langchain_openai import OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from datetime import datetime
from langchain.schema import Document


# === 1. Charger les variables dâ€™environnement ===
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = os.getenv("PINECONE_ENV")
PINECONE_INDEX = os.getenv("PINECONE_INDEX")

# === 2. Initialiser le modÃ¨le ===
llm = ChatOpenAI(temperature=0.5, model="gpt-4o")

# === 3. Initialiser Pinecone retriever ===
retriever = Pinecone.from_existing_index(
    index_name=PINECONE_INDEX,
    embedding=OpenAIEmbeddings()
).as_retriever()


long_term_store = Pinecone.from_existing_index(
    index_name=PINECONE_INDEX,
    embedding=OpenAIEmbeddings(),
    namespace="memory"
)

# === 4. Charger les Ã©vÃ©nements depuis un fichier local ===
def load_evenements_context():
    try:
        with open("evenements_structures.txt", encoding="utf-8") as f:
            return f.read().strip()
    except FileNotFoundError:
        return "Aucun Ã©vÃ©nement n'a pu Ãªtre chargÃ©."

evenements_context = load_evenements_context()

# === 5. Rechercher dans la base de connaissances ===
def get_cci_context(query: str) -> str:
    docs = retriever.invoke(query)
    return "\n\n".join(doc.page_content for doc in docs) if docs else ""

# === 6. Enregistrer un message dans la mÃ©moire long terme ===
def save_to_long_term_memory(text: str, user_id: str):
    now = datetime.utcnow().isoformat()
    doc = Document(
        page_content=text,
        metadata={
            "user_id": user_id,
            "timestamp": now,
            "type": "user_message"
        }
    )
    long_term_store.add_documents([doc])

# === 7. RÃ©cupÃ©rer la mÃ©moire longue dâ€™un utilisateur ===
def retrieve_long_term_memory(query: str, user_id: str) -> str:
    docs = long_term_store.similarity_search(query, k=10)
    filtered_docs = [doc for doc in docs if doc.metadata.get("user_id") == user_id]
    return "\n\n".join(doc.page_content for doc in filtered_docs) if filtered_docs else ""

# === 8. MÃ©moire de conversation courte (session) ===
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory, verbose=False)


# === 6. Fonction principale de lâ€™agent ===
def agent_response(user_input: str, user_id: str) -> str:
    base_cci_context = get_cci_context(user_input)
    long_term_context = retrieve_long_term_memory(user_input, user_id)

    prompt = f"""
Tu es un assistant intelligent et multilingue de la Chambre de Commerce et dâ€™Industrie Franco-mexicaine.
SI un utilisateur te parle en franÃ§ais, rÃ©ponds en franÃ§ais. Si c'est en espagnol, rÃ©ponds en espagnol.
Mission

Voici la mÃ©moire utilisateur long terme :
{long_term_context or '[Aucune mÃ©moire pertinente pour cet utilisateur.]'}

Voici des informations de la base CCI :
{base_cci_context or '[Pas dâ€™information pertinente dans la base.]'}
RÃ©pondre de maniÃ¨re claire, professionnelle et utile Ã  toutes les questions portant sur :

* les services proposÃ©s (accompagnement, formations, Ã©vÃ©nements, networking, soutien aux entreprises, etc.)
* les conditions et avantages dâ€™adhÃ©sion
* les offres rÃ©servÃ©es aux membres
* les partenariats, actualitÃ©s et Ã©vÃ©nements Ã  venir

Bonnes pratiques

* proposer des liens directs vers les pages utiles lorsque câ€™est pertinent
* indiquer oÃ¹ tÃ©lÃ©charger les documents ou formulaires nÃ©cessaires
* si une information nâ€™est pas disponible, le prÃ©ciser avec courtoisie et orienter lâ€™utilisateur vers un contact de la CCI

Langue

* dÃ©tecter automatiquement si la question est posÃ©e en franÃ§ais ou en espagnol
* rÃ©pondre intÃ©gralement dans la langue dÃ©tectÃ©e

Style

* ton professionnel, bienveillant et informatif
* langage clair et structurÃ©, avec des listes Ã  puces si besoin
* ne jamais donner d'information incertaine
* ne jamais sortir du pÃ©rimÃ¨tre de la CCI
  (si la question ne concerne pas la CCI, expliquer poliment que ton rÃ´le est uniquement dâ€™informer sur la CCI et ses services)



L'utilisateur a dit : "{user_input}"

RÃ©ponds en franÃ§ais, de faÃ§on professionnelle, fluide et utile. Si l'utilisateur pose une question simple (bonjour, merci, etc.), rÃ©ponds naturellement sans chercher d'information.
"""

    full_prompt = f"{prompt}\nUtilisateur : {user_input}"
    reply = conversation.predict(input=full_prompt)

    # Enregistrement du message utilisateur dans la mÃ©moire longue
    save_to_long_term_memory(f"Utilisateur : {user_input}\nRÃ©ponse de l'agent : {reply}", user_id=user_id)

    return reply

# === 10. Boucle de chat ===
if __name__ == "__main__":
    print("ğŸ¤– Agent CCI (Ã©vÃ©nements + base vectorielle + mÃ©moire longue) â€” prÃªt !\nTape 'exit' pour quitter.\n")
    
    # This need to be changed to the user id based on the web app
    user_id = "lead_001"
    
    while True:
        user_input = input("ğŸ’¬ Vous : ")
        if user_input.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ Ã€ bientÃ´t !")
            break
        try:
            reply = agent_response(user_input, user_id=user_id)
            print(f"\nğŸ§  Agent :\n{reply}\n")
        except Exception as e:
            print(f"âŒ Erreur : {e}")
